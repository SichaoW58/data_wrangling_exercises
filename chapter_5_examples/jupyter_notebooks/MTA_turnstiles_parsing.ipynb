{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the BeautifulSoup recipe from the bs4 library\n",
    "# this will let us zero in on parts of our webpage based on their class name\n",
    "# and/or tag type\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNCOMMENT BELOW TO USE WITH GOOGLE COLAB\n",
    "# # Import PyDrive and associated libraries.\n",
    "# # This only needs to be done once per notebook.\n",
    "# # Documentation found here: https://colab.research.google.com/notebooks/io.ipynb#scrollTo=7taylj9wpsA2\n",
    "# from pydrive.auth import GoogleAuth\n",
    "# from pydrive.drive import GoogleDrive\n",
    "# from google.colab import auth\n",
    "# from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# # Authenticate and create the PyDrive client.\n",
    "# # This only needs to be done once per notebook.\n",
    "# auth.authenticate_user()\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.credentials = GoogleCredentials.get_application_default()\n",
    "# drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNCOMMENT BELOW TO USE WITH GOOGLE COLAB\n",
    "# # Link to data file stored in Drive: https://drive.google.com/file/d/14TipREniX71e7UDFFcKbyU6C7eZp6G7Y/view?usp=sharing\n",
    "# file_id = '14TipREniX71e7UDFFcKbyU6C7eZp6G7Y' # notice where this string comes from in link above\n",
    "\n",
    "# imported_file = drive.CreateFile({'id': file_id}) # creating an accessible copy of the shared data file\n",
    "# print(imported_file['title'])  # it should print the title of desired file\n",
    "# imported_file.GetContentFile(imported_file['title']) # refer to it in this notebook by the same name as it has in Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the saved copy of our MTA turnstiles webpage, original here:\n",
    "# http://web.mta.info/developers/turnstile.html\n",
    "mta_webpage = open(\"MTA_turnstiles_index.html\", \"r\")\n",
    "\n",
    "# if we click one of the data links on the live copy of the webpage, we\n",
    "# see that the first part of the URL is \"http://web.mta.info/developers/\"\n",
    "# each link only contains \"data/nyct/turnstile/turnstile_YYMMDD.txt\",\n",
    "# so we'll need to combine those links with this base URL for them to work later\n",
    "base_url = \"http://web.mta.info/developers/\"\n",
    "\n",
    "# the BeautifulSoup recipe takes the contents of our webpage and another\n",
    "# \"ingredient\", which tells it what kind of code it is working with. In this\n",
    "# case, it's HTML\n",
    "soup = BeautifulSoup(mta_webpage, \"html.parser\")\n",
    "\n",
    "# using the \"find\" recipe, we can pass a tag type and class name as\n",
    "# \"ingredients\" to zero in on the content we want. Thanks to our work with the\n",
    "# inspection tools, we can go straight to a `div` with the class `span-84 last`\n",
    "# note that because the word \"class\" has a special meaning in Python, the\n",
    "# ingredient name adds an underscore to the end: `class_`\n",
    "data_files_section = soup.find(\"div\", class_=\"span-84 last\")\n",
    "\n",
    "# within that div, we can now just look for all the \"anchor\" tags\n",
    "all_data_links = data_files_section.find_all(\"a\")\n",
    "\n",
    "# need to open a file to write our extracted links to\n",
    "mta_data_list = open(\"MTA_data_index.csv\",\"w\")\n",
    "\n",
    "# the \"find_all()\" recipe returns a list of everything it matches, so we\n",
    "# use a for...in loop to go through all the links\n",
    "for a_link in all_data_links:\n",
    "\n",
    "    # creat a variable that combines our base URL with the contents of the\n",
    "    # \"href\" property of each link (which is where the link information lives)\n",
    "    complete_link = base_url+a_link[\"href\"]\n",
    "\n",
    "    # write this completed link to our output file, manually adding a\n",
    "    # newline `\\n` character to the end, so each link will be on its own row\n",
    "    mta_data_list.write(complete_link+\"\\n\")\n",
    "\n",
    "\n",
    "# once we've written all the links to our file, close it!\n",
    "mta_data_list.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNCOMMENT BELOW TO USE WITH GOOGLE COLAB\n",
    "# from google.colab import files\n",
    "\n",
    "# files.download(\"MTA_data_index.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
