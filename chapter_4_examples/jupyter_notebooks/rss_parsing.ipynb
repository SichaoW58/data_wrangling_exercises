{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "rss_parsing.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYZrG2uquimp"
      },
      "source": [
        "# An example of reading data from a .xml file with Python, using the \"lxml\" library.\n",
        "# First, you'll need pip install the lxml library: https://pypi.org/project/lxml/\n",
        "# The data used here is an instance of\n",
        "# http://feeds.bbci.co.uk/news/science_and_environment/rss.xml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hJppXCkuim2"
      },
      "source": [
        "# specify the particular \"chapter\" you want to import from the \"lxml\" library\n",
        "# in this case, \"etree\", which stands for \"ElementTree\"\n",
        "from lxml import etree\n",
        "\n",
        "# we'll also import the \"csv\" library because we want to convert our workbook\n",
        "# to a `.csv`\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca_KOljKuxo9"
      },
      "source": [
        "# # UNCOMMENT BELOW TO USE WITH GOOGLE COLAB\n",
        "# # Import PyDrive and associated libraries.\n",
        "# # This only needs to be done once per notebook.\n",
        "# # Documentation found here: https://colab.research.google.com/notebooks/io.ipynb#scrollTo=7taylj9wpsA2\n",
        "# from pydrive.auth import GoogleAuth\n",
        "# from pydrive.drive import GoogleDrive\n",
        "# from google.colab import auth\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# # Authenticate and create the PyDrive client.\n",
        "# # This only needs to be done once per notebook.\n",
        "# auth.authenticate_user()\n",
        "# gauth = GoogleAuth()\n",
        "# gauth.credentials = GoogleCredentials.get_application_default()\n",
        "# drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Pnw6oJOuyCO"
      },
      "source": [
        "# # UNCOMMENT BELOW TO USE WITH GOOGLE COLAB\n",
        "# # Link to data file stored in Drive: https://drive.google.com/file/d/1zOaksshLfmXxLTipoOjTTnuO6PsVQgg2/view?usp=sharing\n",
        "# file_id = '1zOaksshLfmXxLTipoOjTTnuO6PsVQgg2' # notice where this string comes from in link above\n",
        "\n",
        "# imported_file = drive.CreateFile({'id': file_id}) # creating an accessible copy of the shared data file\n",
        "# print(imported_file['title'])  # it should print the title of desired file\n",
        "# imported_file.GetContentFile(imported_file['title']) # refer to it in this notebook by the same name as it has in Drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFIE4NRVuim4"
      },
      "source": [
        "# even though there is content inside our RSS we could use to identify the\n",
        "# output file, the easiest thing to do is just give it the same file name\n",
        "# with a `.csv` extension - then we'll know what data source it goes with!\n",
        "filename = \"BBC News - Science & Environment XML Feed\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnBs_zI1uim4"
      },
      "source": [
        "# open is a built-in function that takes two \"ingredients\":\n",
        "# 1. a file name (the file should be in the same folder as this Python script or notebook)\n",
        "# 2. a \"mode\": \"r\" for \"read\" or \"w\" for \"write\"\n",
        "# Because the lxml library expects byte data rather than text, in this case the\n",
        "# second argument \"ingredient\" is \"rb\" for \"read bytes\"\n",
        "xml_source_file = open(filename+\".xml\",\"rb\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79M766HHuim5"
      },
      "source": [
        "# pass our xml_source_file as an ingredient to the the lxml etree library's parse method\n",
        "# and store the result in a variable called `xml_doc`\n",
        "xml_doc = etree.parse(xml_source_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVhZ3-HLuim6"
      },
      "source": [
        "# there is a lot of malformed xml out there! in order to make sure that\n",
        "# what looks like good xml actually *is*, we'll start by getting\n",
        "# the current xml document's \"root\" element\n",
        "document_root = xml_doc.getroot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzS735ZQuim7"
      },
      "source": [
        "# if the document_root is a well-formed XML element, continue with our\n",
        "# wrangling efforts\n",
        "if etree.iselement(document_root):\n",
        "\n",
        "    # create our output file, naming it \"rss_\"+filename\n",
        "    output_file = open(\"rss_\"+filename+\".csv\",\"w\")\n",
        "\n",
        "    # there is a \"writer\" recipe that lets us easily write `.csv`-formatted rows\n",
        "    # we'll use this recipe to easily write rows, instead of reading them\n",
        "    output_writer = csv.writer(output_file)\n",
        "\n",
        "\n",
        "    # as always, we'll want to balance what we handle programmatically and what\n",
        "    # we review visually. In looking at our data, it's clear that each article's\n",
        "    # information is stored in a separate `item` element. since copying over\n",
        "    # the individual tag and attribute names would be time-consuming and error-\n",
        "    # prone, however, we'll go through *one* item element and make a list of\n",
        "    # all the tags (and attributes) within it, which we'll then use as the column\n",
        "    # headers for our output `.csv` file\n",
        "\n",
        "    # document_root[0] is the \"channel\" element\n",
        "    main_channel = document_root[0]\n",
        "\n",
        "    # the `find()` method returns *only* the first instance of the element name\n",
        "    article_example = main_channel.find('item')\n",
        "\n",
        "    # create an empty list in which to store our future column headers\n",
        "    tag_list = []\n",
        "\n",
        "    # iterdescendants() is a method particular to the lxml library, which returns\n",
        "    # *only* the descendants of an element. The more common iter() method would\n",
        "    # return both the element *itself* and its \"descendants\"\n",
        "    # https://lxml.de/api.html#iteration\n",
        "    for child in article_example.iterdescendants():\n",
        "        # child.tag will provide the text of the tagname of the element\n",
        "        # for example, for the <pubDate> element it will return \"pubDate\"\n",
        "        # add each tag to our would-be header list\n",
        "        tag_list.append(child.tag)\n",
        "\n",
        "        # only one tag in our <item> element has an attribute, but we still want\n",
        "        # to include it in our output\n",
        "\n",
        "        # if the current tag has any attributes...\n",
        "        if child.attrib:\n",
        "\n",
        "            # we want the name or \"key\" of the attribute\n",
        "            # the .keys() method will give us a list, so even though there is\n",
        "            # only one attribute, we need to write a `for...in` loop to\n",
        "            # go through and get its name as a string (instead of a one-item list)\n",
        "            # fortunately, this code can be easily reused where we have multiple\n",
        "            # attributes\n",
        "            for attribute_name in child.attrib.keys():\n",
        "\n",
        "                # append the attribut name to our `tag_list` column headers\n",
        "                tag_list.append(attribute_name)\n",
        "\n",
        "\n",
        "    # that whole `article_example` for loop was just to build `tag_list`\n",
        "    # now that we're done, we'll write its contents to our output file as\n",
        "    # column headers\n",
        "    output_writer.writerow(tag_list)\n",
        "\n",
        "\n",
        "    # now we want to grab *every* <item> elment in our file\n",
        "    # so we use the `findall()` method instead of 'find()', as we did above\n",
        "    for item in main_channel.findall('item'):\n",
        "\n",
        "        # empty list for holding our new row's content\n",
        "        new_row = []\n",
        "\n",
        "        # now we'll use our list of tags to get the contents of each element\n",
        "        # within each item\n",
        "        for tag in tag_list:\n",
        "\n",
        "            # if there is anything in the element with a given tag name\n",
        "            if item.findtext(tag):\n",
        "                # append it to our new row\n",
        "                new_row.append(item.findtext(tag))\n",
        "            # otherwise, make sure it's the \"isPermaLink\" attribute\n",
        "            elif tag == \"isPermaLink\":\n",
        "\n",
        "                # and grab its value from the <guid> element\n",
        "                # and append it to our row\n",
        "                new_row.append(item.find('guid').get(\"isPermaLink\"))\n",
        "\n",
        "        # write the new row to our output file!\n",
        "        output_writer.writerow(new_row)\n",
        "\n",
        "    # just for good measure, let's close the `.csv` file we just wrote all that\n",
        "    # data to\n",
        "    output_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H1qy9hCukwd"
      },
      "source": [
        "# # UNCOMMENT BELOW TO USE WITH GOOGLE COLAB\n",
        "# from google.colab import files\n",
        "\n",
        "# files.download(\"rss_\"+filename+\".csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}