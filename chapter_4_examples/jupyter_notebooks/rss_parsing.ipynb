{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of reading data from a .xml file with Python, using the \"lxml\" library.\n",
    "# First, you'll need pip install the lxml library: https://pypi.org/project/lxml/\n",
    "# The data used here is an instance of\n",
    "# http://feeds.bbci.co.uk/news/science_and_environment/rss.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the particular \"chapter\" you want to import from the \"lxml\" library\n",
    "# in this case, \"etree\", which stands for \"ElementTree\"\n",
    "from lxml import etree\n",
    "\n",
    "# we'll also import the \"csv\" library because we want to convert our workbook\n",
    "# to a `.csv`\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even though there is content inside our RSS we could use to identify the\n",
    "# output file, the easiest thing to do is just give it the same file name\n",
    "# with a `.csv` extension - then we'll know what data source it goes with!\n",
    "filename = \"BBC News - Science & Environment XML Feed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open is a built-in function that takes two \"ingredients\":\n",
    "# 1. a file name (the file should be in the same folder as this Python script or notebook)\n",
    "# 2. a \"mode\": \"r\" for \"read\" or \"w\" for \"write\"\n",
    "# Because the lxml library expects byte data rather than text, in this case the\n",
    "# second argument \"ingredient\" is \"rb\" for \"read bytes\"\n",
    "xml_source_file = open(filename+\".xml\",\"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass our xml_source_file as an ingredient to the the lxml etree library's parse method\n",
    "# and store the result in a variable called `xml_doc`\n",
    "xml_doc = etree.parse(xml_source_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a lot of malformed xml out there! in order to make sure that\n",
    "# what looks like good xml actually *is*, we'll start by getting\n",
    "# the current xml document's \"root\" element\n",
    "document_root = xml_doc.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the document_root is a well-formed XML element, continue with our\n",
    "# wrangling efforts\n",
    "if etree.iselement(document_root):\n",
    "\n",
    "    # create our output file, naming it \"rss_\"+filename\n",
    "    output_file = open(\"rss_\"+filename+\".csv\",\"w\")\n",
    "\n",
    "    # there is a \"writer\" recipe that lets us easily write `.csv`-formatted rows\n",
    "    # we'll use this recipe to easily write rows, instead of reading them\n",
    "    output_writer = csv.writer(output_file)\n",
    "\n",
    "\n",
    "    # as always, we'll want to balance what we handle programmatically and what\n",
    "    # we review visually. In looking at our data, it's clear that each article's\n",
    "    # information is stored in a separate `item` element. since copying over\n",
    "    # the individual tag and attribute names would be time-consuming and error-\n",
    "    # prone, however, we'll go through *one* item element and make a list of\n",
    "    # all the tags (and attributes) within it, which we'll then use as the column\n",
    "    # headers for our output `.csv` file\n",
    "\n",
    "    # document_root[0] is the \"channel\" element\n",
    "    main_channel = document_root[0]\n",
    "\n",
    "    # the `find()` method returns *only* the first instance of the element name\n",
    "    article_example = main_channel.find('item')\n",
    "\n",
    "    # create an empty list in which to store our future column headers\n",
    "    tag_list = []\n",
    "\n",
    "    # iterdescendants() is a method particular to the lxml library, which returns\n",
    "    # *only* the descendants of an element. The more common iter() method would\n",
    "    # return both the element *itself* and its \"descendants\"\n",
    "    # https://lxml.de/api.html#iteration\n",
    "    for child in article_example.iterdescendants():\n",
    "        # child.tag will provide the text of the tagname of the element\n",
    "        # for example, for the <pubDate> element it will return \"pubDate\"\n",
    "        # add each tag to our would-be header list\n",
    "        tag_list.append(child.tag)\n",
    "\n",
    "        # only one tag in our <item> element has an attribute, but we still want\n",
    "        # to include it in our output\n",
    "\n",
    "        # if the current tag has any attributes...\n",
    "        if child.attrib:\n",
    "\n",
    "            # we want the name or \"key\" of the attribute\n",
    "            # the .keys() method will give us a list, so even though there is\n",
    "            # only one attribute, we need to write a `for...in` loop to\n",
    "            # go through and get its name as a string (instead of a one-item list)\n",
    "            # fortunately, this code can be easily reused where we have multiple\n",
    "            # attributes\n",
    "            for attribute_name in child.attrib.keys():\n",
    "\n",
    "                # append the attribut name to our `tag_list` column headers\n",
    "                tag_list.append(attribute_name)\n",
    "\n",
    "\n",
    "    # that whole `article_example` for loop was just to build `tag_list`\n",
    "    # now that we're done, we'll write its contents to our output file as\n",
    "    # column headers\n",
    "    output_writer.writerow(tag_list)\n",
    "\n",
    "\n",
    "    # now we want to grab *every* <item> elment in our file\n",
    "    # so we use the `findall()` method instead of 'find()', as we did above\n",
    "    for item in main_channel.findall('item'):\n",
    "\n",
    "        # empty list for holding our new row's content\n",
    "        new_row = []\n",
    "\n",
    "        # now we'll use our list of tags to get the contents of each element\n",
    "        # within each item\n",
    "        for tag in tag_list:\n",
    "\n",
    "            # if there is anything in the element with a given tag name\n",
    "            if item.findtext(tag):\n",
    "                # append it to our new row\n",
    "                new_row.append(item.findtext(tag))\n",
    "            # otherwise, make sure it's the \"isPermaLink\" attribute\n",
    "            elif tag == \"isPermaLink\":\n",
    "\n",
    "                # and grab its value from the <guid> element\n",
    "                # and append it to our row\n",
    "                new_row.append(item.find('guid').get(\"isPermaLink\"))\n",
    "\n",
    "        # write the new row to our output file!\n",
    "        output_writer.writerow(new_row)\n",
    "\n",
    "    # just for good measure, let's close the `.csv` file we just wrote all that\n",
    "    # data to\n",
    "    output_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
